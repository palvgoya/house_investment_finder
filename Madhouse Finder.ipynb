{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Geopy to calculate distances between points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy import distance, geocoders, Nominatim\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all Datasets saved on csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mad_houses=pd.read_csv('df_mad_houses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mad_houses.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mad_postcode_latlon= pd.read_csv('latlons_by_postcode_madrid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the postcode file to include city and country to be able to make searches to the Nominatim API and get the latlons of each postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mad_postcode_latlon['address']=list(zip(df_mad_postcode_latlon['codigopostalid'],df_mad_postcode_latlon['city'],df_mad_postcode_latlon['country']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mad_postcode_latlon=df_mad_postcode_latlon.drop(columns=['Unnamed: 0','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "user_agent = 'palvgoya@hotmail.com'\n",
    "geolocator = Nominatim(user_agent=user_agent)\n",
    "\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "df_mad_postcode_latlon['location'] = df_mad_postcode_latlon['address'].apply(geocode)\n",
    "\n",
    "df_mad_postcode_latlon['location'] = df_mad_postcode_latlon['location'].apply(lambda loc: tuple(loc.point) if loc else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "df_mad_postcode_latlon[['lat', 'lon', 'alt']] = pd.DataFrame(df_mad_postcode_latlon['location'].tolist(), index=df_mad_postcode_latlon.index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "df_mad_postcode_latlon['latlon_merged']=list(zip(df_mad_postcode_latlon['lat'],df_mad_postcode_latlon['lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mad_postcode_latlon.to_csv('latlons_by_postcode_madrid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "df_mad_postcode_latlon=df_mad_postcode_latlon.drop(columns=['city','country','address','location','alt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mad_postcode_latlon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the houses with the latlons file to get latlons of each house by postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mad_houses=df_mad_houses.merge(df_mad_postcode_latlon,left_on='post_code',right_on='codigopostalid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate distance of each house to closest transport station\n",
    "\n",
    "Calculating closest transport stop to center point of each neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport_stops=pd.read_csv('transport_stops_madrid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport_stops.drop(columns=['stop_id', 'stop_code', 'stop_name', 'stop_desc', 'zone_id','stop_url', 'location_type', 'parent_station', 'stop_timezone', 'wheelchair_boarding'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport_stops['stop_loc'] = list(zip(df_transport_stops['stop_lat'], df_transport_stops['stop_lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "dist_transport=[]\n",
    "\n",
    "for a in range(0,26599):\n",
    "    point = df_mad_houses['latlon_merged'][a]\n",
    "    dist = df_transport_stops['stop_loc'].apply(lambda x: distance.geodesic(x, point).m).min()\n",
    "    dist_transport.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_mad_houses['transport']=dist_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Talamanca de Jarama', ['lat','lon','latlon_merged']] = '40.7460256','-3.5129914','(40.7460256, -3.5129914)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Belmonte de Tajo', ['lat','lon','latlon_merged']] = '40.1336305','-3.3398654','(40.1336305, -3.3398654)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Carabaña', ['lat','lon','latlon_merged']] = '40.256626','-3.2347206','(40.256626, -3.2347206)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Robledo de Chavela', ['lat','lon','latlon_merged']] = '40.5009201','-4.2380999','(40.5009201, -4.2380999)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'La Cabrera', ['lat','lon','latlon_merged']] = '40.8651738','-3.6125468','(40.8651738, -3.6125468)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Ribatejada', ['lat','lon','latlon_merged']] = '40.6664949','-3.3919178','(40.6664949, -3.3919178)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Villa del Prado', ['lat','lon','latlon_merged']] = '40.2764752','-4.3048032','(40.2764752, -4.3048032)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Torrelaguna', ['lat','lon','latlon_merged']] = '40.8277273','-3.538293','(40.8277273, -3.538293)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Santa María de la Alameda', ['lat','lon','latlon_merged']] = '40.5958248','-4.2579208','(40.5958248, -4.2579208)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Valdemanco', ['lat','lon','latlon_merged']] = '40.863336950000004','-3.6624598591227406','(40.863336950000004, -3.6624598591227406)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'El Vellón', ['lat','lon','latlon_merged']] = '40.7677965','-3.5820099','(40.7677965, -3.5820099)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Aldea del Fresno', ['lat','lon','latlon_merged']] = '40.3215212','-4.19936','(40.3215212, -4.19936)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Cubas de la Sagra', ['lat','lon','latlon_merged']] = '40.1908352','-3.8378366', '(40.1908352, -3.8378366)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Perales de Tajuña', ['lat','lon','latlon_merged']] = '40.2325718','-3.3517265', '(40.2325718, -3.3517265)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Valdilecha', ['lat','lon','latlon_merged']] = '40.2926854','-3.3002096', '(40.2926854, -3.3002096)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Bustarviejo', ['lat','lon','latlon_merged']] = '40.8586659','-3.7101535', '(40.8586659, -3.7101535)'\n",
    "df_mad_houses.loc[df_mad_houses['poblacion'] == 'Getafe', ['lat','lon','latlon_merged']] = '40.3081807','-3.7302679', '(40.3081807, -3.7302679)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mad_houses['latlon_merged']=list(zip(df_mad_houses['lat'],df_mad_houses['lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mad_houses.to_csv('df_mad_houses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate distance of each house to closest school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schools=pd.read_csv('Adresses_Schools_Mad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "geolocator = geocoders.ArcGIS()\n",
    "\n",
    "df_schools['location'] = df_schools['address'].apply(geolocator.geocode,timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "df_schools[['a','latlon']]= pd.DataFrame(df_schools['location'].to_list(),index=df_schools.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "df_schools.drop(columns=['a','address','random1','random2','Domicilio','Código'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_schools.to_csv('schools_mad_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "dist_school=[]\n",
    "\n",
    "for a in range(0,26599):\n",
    "    point = df_mad_houses['latlon_merged'][a]\n",
    "    dist = df_schools['latlon'].apply(lambda x: distance.geodesic(x, point).m).min()\n",
    "    dist_school.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mad_houses['school']=dist_school"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate distance of each house to closest health centre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we take the hospitals file and get the latlon using geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hospitals=pd.read_csv('salud_madrid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hospitals['address']= df_hospitals['address'].str.replace('[', ',')\n",
    "df_hospitals['address']= df_hospitals['address'].str.replace(']', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "df_hospitals=df_hospitals.drop(columns=['direccion','random1','random2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "geolocator = geocoders.ArcGIS()\n",
    "\n",
    "df_hospitals['location'] = df_hospitals['address'].apply(geolocator.geocode,timeout=10)\n",
    "\n",
    "df_hospitals[['place','latlon']] = pd.DataFrame(df_hospitals['location'].tolist(), index=df_hospitals.index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_hospitals.to_csv('salud_madrid_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "dist_health_centre=[]\n",
    "\n",
    "for a in range(0,26599):\n",
    "    point = df_mad_houses['latlon_merged'][a]\n",
    "    dist = df_hospitals['latlon'].apply(lambda x: distance.geodesic(x, point).m).min()\n",
    "    dist_health_centre.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mad_houses['health_centre']=dist_health_centre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Load past data to start training and estimate neighbourhood prices per m2 in 2030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_past_mad= pd.read_csv('dataset_16_19_mad_3.csv')\n",
    "df_past_mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_past_mad['year']=pd.to_datetime(df_past_mad['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index=range(0,157)\n",
    "columns=['neighbourhood','transactions','avgsize','year_cat']\n",
    "\n",
    "df_pred_past=pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "df_pred_past['year_cat']=14\n",
    "df_pred_past['neighbourhood']=df_past_mad['neighbourhood']\n",
    "df_pred_past['transactions']=df_past_mad.groupby('neighbourhood')['transactions'].mean().values\n",
    "df_pred_past['avgsize']=df_past_mad.groupby('neighbourhood')['avgsize'].mean().values\n",
    "\n",
    "onehot_enc_pred = pd.get_dummies(df_pred_past['neighbourhood'])\n",
    "\n",
    "df_pred_past = pd.concat([df_pred_past, onehot_enc_pred], axis=1, join=\"inner\")\n",
    "\n",
    "df_pred_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Assigning numerical values to year and storing in another column to create a category\n",
    "df_past_mad['year_cat'] = labelencoder.fit_transform(df_past_mad['year'])\n",
    "df_past_mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past_mad['transactions'] = df_past_mad['transactions'].replace('-','0')\n",
    "df_past_mad['avgsize'] = df_past_mad['avgsize'].replace('-','0')\n",
    "df_past_mad['avgsize'] = df_past_mad['avgsize'].replace(',','.')\n",
    "df_past_mad['avg_eur_m2'] = df_past_mad['avg_eur_m2'].replace('-','0')\n",
    "df_past_mad['avg_eur_m2'] = df_past_mad['avg_eur_m2'].replace(',','.')\n",
    "\n",
    "df_past_mad=df_past_mad.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_enc = pd.get_dummies(df_past_mad['neighbourhood'])\n",
    "\n",
    "df_past_mad = pd.concat([df_past_mad, onehot_enc], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_past_mad[['transactions','avgsize']] = scaler.fit_transform(df_past_mad[['transactions','avgsize']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pred_past[['transactions','avgsize']] = scaler.fit_transform(df_pred_past[['transactions','avgsize']])\n",
    "\n",
    "df_pred_past= df_pred_past.drop(columns='neighbourhood')\n",
    "\n",
    "df_pred_past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We split the train data into features and target and start the machine learning modelling to get the future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(df_past_mad['avg_eur_m2'])\n",
    "X= df_past_mad.drop(columns=['avg_eur_m2','year','neighbourhood'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.21,random_state=37,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start testing different models using this graph and moving towards the ones that gives better R2 rates\n",
    "***\n",
    "    \n",
    "![Regression Models](https://miro.medium.com/max/1674/1*_Wx0vKokbXd20HlbLKpj2A.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr= lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse= mean_squared_error(y_test,predictions_lr,squared=False)\n",
    "r2_confidence = r2_score(y_test,predictions_lr)\n",
    "mae=mean_absolute_error(y_test,predictions_lr)\n",
    "mse=mean_squared_error(y_test,predictions_lr)\n",
    "print('RMSE: ', rmse)\n",
    "print('R2 confidence: ', r2_confidence)\n",
    "print('MAE: ', mae)\n",
    "print('MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the data with a Lasso Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=Lasso(alpha=0.1)\n",
    "ls.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ls= ls.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse= mean_squared_error(y_test,predictions_ls,squared=False)\n",
    "r2_confidence = r2_score(y_test,predictions_ls)\n",
    "mae=mean_absolute_error(y_test,predictions_ls)\n",
    "mse=mean_squared_error(y_test,predictions_ls)\n",
    "print('RMSE: ', rmse)\n",
    "print('R2 confidence: ', r2_confidence)\n",
    "print('MAE: ', mae)\n",
    "print('MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using a Ridge Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd=Ridge(alpha=0.1)\n",
    "rd.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rd=rd.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse= mean_squared_error(y_test,predictions_rd,squared=False)\n",
    "r2_confidence = r2_score(y_test,predictions_rd)\n",
    "mae=mean_absolute_error(y_test,predictions_rd)\n",
    "mse=mean_squared_error(y_test,predictions_rd)\n",
    "print('RMSE: ', rmse)\n",
    "print('R2 confidence: ', r2_confidence)\n",
    "print('MAE: ', mae)\n",
    "print('MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import TheilSenRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_reg=TheilSenRegressor()\n",
    "ts_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ts_reg=ts_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse= mean_squared_error(y_test,predictions_ts_reg,squared=False)\n",
    "r2_confidence = r2_score(y_test,predictions_ts_reg)\n",
    "mae=mean_absolute_error(y_test,predictions_ts_reg)\n",
    "mse=mean_squared_error(y_test,predictions_ts_reg)\n",
    "print('RMSE: ', rmse)\n",
    "print('R2 confidence: ', r2_confidence)\n",
    "print('MAE: ', mae)\n",
    "print('MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr=SVR(kernel='linear', C=100, gamma=0.1)\n",
    "svr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svr=svr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse= mean_squared_error(y_test,predictions_svr,squared=False)\n",
    "r2_confidence = r2_score(y_test,predictions_svr)\n",
    "mae=mean_absolute_error(y_test,predictions_svr)\n",
    "mse=mean_squared_error(y_test,predictions_svr)\n",
    "print('RMSE: ', rmse)\n",
    "print('R2 confidence: ', r2_confidence)\n",
    "print('MAE: ', mae)\n",
    "print('MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly using Random Forest regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_for=RandomForestRegressor(n_estimators=300,max_features='log2',max_depth=70,random_state=42)\n",
    "random_for.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_random_for=random_for.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse= mean_squared_error(y_test,predictions_random_for,squared=False)\n",
    "r2_confidence = r2_score(y_test,predictions_random_for)\n",
    "mae=mean_absolute_error(y_test,predictions_random_for)\n",
    "mse=mean_squared_error(y_test,predictions_random_for)\n",
    "print('RMSE: ', rmse)\n",
    "print('R2 confidence: ', r2_confidence)\n",
    "print('MAE: ', mae)\n",
    "print('MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## After testing three different models we see that the Linear Regression model is giving the best results in terms of scoring so we´ll use that one to estimate the future price of each neighbourhood/district in Madrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr_future=lr.predict(df_pred_past)\n",
    "\n",
    "df_pred_past['future_price_m2']=pred_lr_future\n",
    "\n",
    "df_pred_past['neighbourhood']=df_past_mad['neighbourhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=range(0,157)\n",
    "columns=['neighbourhood','price_pred_2030']\n",
    "\n",
    "price_pred_2030=pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "price_pred_2030['neighbourhood']=df_past_mad['neighbourhood']\n",
    "price_pred_2030['price_pred_2030']=pred_lr_future\n",
    "\n",
    "price_pred_2030"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Now we load again the file we have with all the houses on the market in Madrid and add the estimated future price per m2 by district to get an estimation of the return of investment (ROI) per house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_pred_2030['neighbourhood'] = price_pred_2030['neighbourhood'].str.replace('M-','')\n",
    "price_pred_2030['neighbourhood'] = price_pred_2030['neighbourhood'].str.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postal_code=pd.read_csv('listado_codigos_postales_mad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postal_code['poblacion']=postal_code['poblacion'].str.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_future_prices=pd.merge(postal_code,price_pred_2030,left_on='poblacion',right_on='neighbourhood',how='left',validate='many_to_one')\n",
    "df_future_prices.drop(columns=['provincia','poblacion','lat','lon','neighbourhood'], inplace=True)\n",
    "df_future_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_houses=pd.merge(df_mad_houses,df_future_prices,left_on='post_code',right_on='codigopostalid',how='left',validate='many_to_many')\n",
    "\n",
    "df_final_houses['rooms'] = df_final_houses['rooms'].str.replace(' hab','')\n",
    "df_final_houses['bathrooms'] = df_final_houses['bathrooms'].str.replace(' wc','')\n",
    "df_final_houses['size'] = df_final_houses['size'].str.replace(' m²','')\n",
    "df_final_houses['prices'] = df_final_houses['prices'].str.replace('.',',')\n",
    "df_final_houses['prices'] = df_final_houses['prices'].str.replace(',','')\n",
    "df_final_houses['prices'] = df_final_houses['prices'].str.replace('€','')\n",
    "df_mad_houses['poblacion'] = df_mad_houses['poblacion'].str.replace(' ','')\n",
    "\n",
    "df_final_houses['rooms'] = df_final_houses['rooms'].str.replace(' hab','')\n",
    "df_final_houses['bathrooms'] = df_final_houses['bathrooms'].str.replace(' wc','')\n",
    "df_final_houses['size'] = df_final_houses['size'].str.replace(' m²','')\n",
    "df_final_houses['prices'] = df_final_houses['prices'].str.replace('.',',')\n",
    "df_final_houses['prices'] = df_final_houses['prices'].str.replace(',','')\n",
    "df_final_houses['prices'] = df_final_houses['prices'].str.replace('€','')\n",
    "df_mad_houses['poblacion'] = df_mad_houses['poblacion'].str.replace(' ','')\n",
    "\n",
    "df_final_houses.drop(columns=['codigopostalid'],inplace=True)\n",
    "\n",
    "df_final_houses['prices']=pd.to_numeric(df_final_houses['prices'], downcast='float')\n",
    "df_final_houses['rooms']=pd.to_numeric(df_final_houses['rooms'])\n",
    "df_final_houses['bathrooms']=pd.to_numeric(df_final_houses['bathrooms'])\n",
    "df_final_houses['size']=pd.to_numeric(df_final_houses['size'])\n",
    "\n",
    "df_final_houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_houses[df_final_houses['prices']==2900000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_houses[df_final_houses['rooms']>=9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_houses=df_final_houses.drop([19540, 19526, 25267, 3485, 21002, 21001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_growth_median= ((df_final_houses['price_pred_2030']-(df_final_houses['prices']/df_final_houses['size']))/(df_final_houses['prices']/df_final_houses['size'])).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_growth_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a calculation for small neighbourhoods where we don´t have an estimation of future price. \n",
    "#And apply that growth median to the value in 2021\n",
    "df_final_houses['price_pred_2030'].fillna((df_final_houses['prices']/df_final_houses['size'])*(1+price_growth_median),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all= df_final_houses[['post_code','rooms','bathrooms','size','transport','school','health_centre']]\n",
    "y_all= df_final_houses['prices']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we analyse the data to convert the Transport, School and Health Centre distances columns into categorical values to avoid the ML algorith taking this values into account generating unbalanced data.\n",
    "In order to do that, we need to generate 3 ranges from close to far disance to points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_houses['transport'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(df_final_houses['transport'], 4).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_houses['school'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(df_final_houses['school'], 4).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_houses['health_centre'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(df_final_houses['health_centre'], 4).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bearing this in mind we create different scores for distances 0 to 500meters, 500 to 1000, 1000 to 2000 and over 2000 which is over 2kms to the closest health centre, school and transport respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.loc[X_all['transport']<=500,'trans_score']= 1\n",
    "X_all.loc[(X_all['transport']>=501) & (X_all['transport']<=1000),'trans_score']= 2\n",
    "X_all.loc[(X_all['transport']>=1001) & (X_all['transport']<=2000),'trans_score']= 3\n",
    "X_all.loc[(X_all['transport']>=2001),'trans_score']= 4\n",
    "\n",
    "X_all.loc[X_all['school']<=500,'school_score']= 1\n",
    "X_all.loc[(X_all['school']>=501) & (X_all['school']<=1000),'school_score']= 2\n",
    "X_all.loc[(X_all['school']>=1001) & (X_all['school']<=2000),'school_score']= 3\n",
    "X_all.loc[(X_all['school']>=2001),'school_score']= 4\n",
    "\n",
    "X_all.loc[X_all['health_centre']<=500,'health_score']= 1\n",
    "X_all.loc[(X_all['health_centre']>=501) & (X_all['health_centre']<=1000),'health_score']= 2\n",
    "X_all.loc[(X_all['health_centre']>=1001) & (X_all['health_centre']<=2000),'health_score']= 3\n",
    "X_all.loc[(X_all['health_centre']>=2001),'health_score']= 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use get_dummies to create a onehotencoder of the different postcodes so we have all those categories into different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_zipcode = pd.get_dummies(X_all['post_code'])\n",
    "\n",
    "X_all = pd.concat([X_all, onehot_zipcode], axis=1, join=\"inner\")\n",
    "\n",
    "X_all = X_all.drop(columns=['post_code','transport','school', 'health_centre'])\n",
    "\n",
    "X_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And change the size column to a range 1 to 4 to be aligned with the other features of the ML dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_1_4 = MinMaxScaler(feature_range=(1, 4))\n",
    "\n",
    "X_all[['size']]= scaler_1_4.fit_transform(X_all[['size']])\n",
    "\n",
    "X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added shuffleling becasue the data at the moment is organized by area so we need to take data from different parts of the dataframe\n",
    "trainall_x, testall_x, trainall_y, testall_y = train_test_split(X_all,y_all, test_size= 0.2, random_state=42, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2=LinearRegression()\n",
    "\n",
    "lr2.fit(trainall_x,trainall_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred_y_all=lr2.predict(testall_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(testall_y,lr_pred_y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the Linear Regression are not very accurate this time so we use a Pipeline to be able to normalize the data and do a cross validation to find the best parameters for three different regression models, Lasso, Ridge and RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the three different model objects\n",
    "normalizer = Normalizer()\n",
    "lasso = Lasso ()\n",
    "ridge = Ridge ()\n",
    "rf_reg = RandomForestRegressor (n_jobs=-2)\n",
    "\n",
    "# Now we do a pipeline, first to normalize de data with normalizer to try to speed up the process\n",
    "#and then train the data with each regressor building three different pipelines\n",
    "pipe1 = Pipeline([('normalizer', normalizer), ('lasso', lasso)])\n",
    "\n",
    "pipe2 = Pipeline([('normalizer', normalizer), ('ridge', ridge)])\n",
    "\n",
    "pipe3 = Pipeline([('rf_reg', rf_reg)])\n",
    "\n",
    "# Creating parameters space:\n",
    "\n",
    "# Creating lists of parameters for the Lasso Regression\n",
    "alpha_ls = np.logspace(-3,0.1,25)\n",
    "\n",
    "# Creating lists of parameters for the Ridge Regression\n",
    "alpha_rd = np.logspace(-3,0.1,25)\n",
    "\n",
    "# Creating lists of parameters for the RandomForest Regression\n",
    "min_samples_leaf = list([1,2,3])\n",
    "max_features = ['sqrt', 'auto', 'log2']\n",
    "n_estimators = list([50, 100, 200, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# Creating a dictionary of all the parameter options using '__'\n",
    "param_ls = {'lasso__alpha': alpha_ls}\n",
    "reg1 = GridSearchCV (pipe1, param_grid= param_ls)\n",
    "reg1.fit(trainall_x, trainall_y)\n",
    "\n",
    "print(reg1.best_params_)\n",
    "print(reg1.best_estimator_)\n",
    "print('R2 score is:', reg1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "param_rd = {'ridge__alpha': alpha_rd, 'ridge__fit_intercept': fit_intercept}\n",
    "reg2 = GridSearchCV (pipe2, param_grid= param_rd)\n",
    "reg2.fit(trainall_x, trainall_y)\n",
    "\n",
    "print(reg2.best_params_)\n",
    "print(reg2.best_estimator_)\n",
    "print('R2 score is:', reg2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "param_grid3 = {'rf_reg__n_estimators': n_estimators,\n",
    "               'rf_reg__max_features': max_features}\n",
    "reg3 = GridSearchCV (pipe3, param_grid3)\n",
    "reg3.fit(trainall_x, trainall_y)\n",
    "\n",
    "print(reg3.best_params_)\n",
    "print(reg3.best_estimator_)\n",
    "print('R2 score is:', reg3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing different hyperparameters in three regression models we found that Random Forest is the best model with the parameters shown on the result of last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use the Lasso parameters from the cross validation model\n",
    "\n",
    "rf_reg2=RandomForestRegressor(max_features= 'sqrt', n_estimators= 300, min_samples_leaf= 1, n_jobs=-2)\n",
    "\n",
    "rf_reg2.fit(trainall_x,trainall_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_y_all=rf_reg2.predict(testall_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(testall_y, rf_pred_y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions over this new trained model to compare two different ways to measure profit:<p>\n",
    "    \n",
    "<ol>\n",
    "    <li> Past Neighbouthood data\n",
    "        <li> Existing market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_market_pred= rf_reg2.predict(X_all)\n",
    "\n",
    "df_final_houses['pred_2020_market']=existing_market_pred/df_final_houses['size']\n",
    "\n",
    "df_final_houses['market_per_m2']=df_final_houses['prices']/df_final_houses['size']\n",
    "\n",
    "df_final_houses['profit_now']= (df_final_houses['pred_2020_market']-df_final_houses['market_per_m2'])/df_final_houses['market_per_m2']\n",
    "\n",
    "df_final_houses['profit_future']= (df_final_houses['price_pred_2030']-df_final_houses['market_per_m2'])/df_final_houses['market_per_m2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = df_final_houses[['post_code', 'zona', 'poblacion','rooms', 'bathrooms', 'size', 'prices', 'market_per_m2', 'pred_2020_market', 'price_pred_2030', 'profit_now', 'profit_future']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by=['profit_now','profit_future'], axis= 0, ascending= False, inplace=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Now we start plotting different graphs to analize the data and the results of our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.boxplot(x= 'zona',y= 'prices',data= results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_enc_zones = pd.get_dummies(results_df['zona'])\n",
    "\n",
    "df_houses_corr = pd.concat([df_final_houses, onehot_enc_zones], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12,10))\n",
    "corr = df_houses_corr.corr()\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download folium to be able to plot maps and Cloropleths\n",
    "!pip install folium geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "mad_map= gpd.read_file('https://raw.githubusercontent.com/inigoflores/ds-codigos-postales/d3036e99d124582b1c5c69660bd8d1c6bd0b7af0/data/MADRID.geojson')\n",
    "\n",
    "print(type(mad_map))\n",
    "\n",
    "mad_map.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_map['COD_POSTAL']=mad_map['COD_POSTAL'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_map.sort_values(by='COD_POSTAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4_maps=results_df.groupby(by='post_code').median()\n",
    "df_4_maps.reset_index(inplace=True)\n",
    "\n",
    "df_4_maps['post_code']=df_4_maps['post_code'].astype(int)\n",
    "df_4_maps['rooms']=df_4_maps['rooms'].astype(int)\n",
    "df_4_maps['bathrooms']=df_4_maps['bathrooms'].astype(int)\n",
    "\n",
    "df_4_maps.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final_houses['prices']=df_final_houses['prices'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphmap_mad= folium.Map(location=[40.4881, -3.6683], zoom_start=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_mkt = list(df_4_maps[\"market_per_m2\"].quantile([0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1]))\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=mad_map,\n",
    "    name=\"mkt_price\",\n",
    "    data=df_4_maps,\n",
    "    columns=[\"post_code\", \"market_per_m2\"],\n",
    "    key_on=\"feature.properties.COD_POSTAL\",\n",
    "    fill_color=\"YlOrRd\",\n",
    "    nan_fill_color='#fff7bc',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.6,\n",
    "    line_color='white',\n",
    "    bins=bins_mkt,\n",
    "    highlight=True,\n",
    "    reset=True,\n",
    "    show=False,\n",
    "    control=True\n",
    ").add_to(graphmap_mad)\n",
    "\n",
    "#folium.LayerControl().add_to(graphmap_mad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_pred = list(df_4_maps[\"pred_2020_market\"].quantile([0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1]))\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=mad_map,\n",
    "    name=\"mkt_prediction\",\n",
    "    data=df_4_maps,\n",
    "    columns=[\"post_code\", \"pred_2020_market\"],\n",
    "    key_on=\"feature.properties.COD_POSTAL\",\n",
    "    fill_color=\"YlOrRd\",\n",
    "    nan_fill_color='#fff7bc',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.6,\n",
    "    line_color='white',\n",
    "    bins=bins_pred,\n",
    "    highlight=True,\n",
    "    reset=True,\n",
    "    show=False,\n",
    "    control=True\n",
    ").add_to(graphmap_mad)\n",
    "\n",
    "#folium.LayerControl().add_to(graphmap_mad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_profit_n = list(df_4_maps[\"profit_now\"].quantile([0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1]))\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=mad_map,\n",
    "    name=\"profit_now\",\n",
    "    data=df_4_maps,\n",
    "    columns=[\"post_code\", \"profit_now\"],\n",
    "    key_on=\"feature.properties.COD_POSTAL\",\n",
    "    fill_color=\"YlOrRd\",\n",
    "    nan_fill_color='#fff7bc',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.6,\n",
    "    line_color='white',\n",
    "    bins=bins_profit_n,\n",
    "    highlight=True,\n",
    "    reset=True,\n",
    "    show=False,\n",
    "    control=True\n",
    ").add_to(graphmap_mad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_2030 = list(df_4_maps[\"price_pred_2030\"].quantile([0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1]))\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=mad_map,\n",
    "    name=\"2030_prediction\",\n",
    "    data=df_4_maps,\n",
    "    columns=[\"post_code\", \"price_pred_2030\"],\n",
    "    key_on=\"feature.properties.COD_POSTAL\",\n",
    "    fill_color=\"YlOrRd\",\n",
    "    nan_fill_color='#fff7bc',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.6,\n",
    "    line_color='white',\n",
    "    bins=bins_2030,\n",
    "    highlight=True,\n",
    "    reset=True,\n",
    "    show=False,\n",
    "    control=True\n",
    ").add_to(graphmap_mad)\n",
    "\n",
    "folium.LayerControl(collapsed=False).add_to(graphmap_mad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graphmap_mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = df_final_houses.groupby(by='post_code').median()\n",
    "testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki=df_final_houses.groupby(by=['zona',pd.cut(df_final_houses['size'], 7, labels=False)]).median()\n",
    "wiki=wiki.unstack(level=[-1,-1])\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "#plt.axis([50, 100, 0, 500000])\n",
    "sns.boxplot(x= 'size',y= 'value',data= pd.melt(wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
